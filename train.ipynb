{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import *\n",
    "\n",
    "from utils.dataset import *\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import utils.transforms  as T\n",
    "import os\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DebugImages(running_count, images):\n",
    "    # denormalize imagenet\n",
    "    images = images * torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).cuda()\n",
    "    images = images + torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).cuda()\n",
    "\n",
    "    # save image\n",
    "    torchvision.utils.save_image(images, f\"debug_images/{running_count}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTensor divide255: True\n",
      "ToTensor divide255: True\n"
     ]
    }
   ],
   "source": [
    "image_count = 5\n",
    "latent_size = 512\n",
    "is_dynamic = True\n",
    "sample_count = None\n",
    "aggregation_simple = False\n",
    "embeddings_file_path = \"deepsdf_latent_codes/deepsdf_generalization_final_512v2/latent_best.ckpt\"\n",
    "\n",
    "train_dataset = CustomDataset(image_folder_paths= \"D:/TUM/ML43D_Project/TrainDataSet\",\n",
    "                            embeddings_file_path=embeddings_file_path, \n",
    "                            embedding_name_list_file_path= \"data/embeddings/obj_files.json\",\n",
    "                            embedding_size=latent_size,\n",
    "                            image_count=image_count,\n",
    "                            sample_count=sample_count,\n",
    "                            dynamic=is_dynamic,\n",
    "                            transform=transforms.Compose([\n",
    "                                T.Resize(224, 224, image_count),\n",
    "                                #T.RandomCrop(224, image_count),\n",
    "                                T.GaussianBlur(0.1, 5, image_count),\n",
    "                                T.Noise(0.1, (-20,-20,-20),(20,20,20), 15, image_count),\n",
    "                                #T.Rotation(-5, 5, 0.4, image_count),\n",
    "                                T.SwitchRGB(0.1, image_count),\n",
    "                                OmniObject.ToTensor(divide255=True, img_count=image_count)\n",
    "                                ])\n",
    "                            )\n",
    "    \n",
    "\n",
    "val_dataset = CustomDataset(image_folder_paths= \"D:/TUM/ML43D_Project/ValidationDataSet\",\n",
    "                            embeddings_file_path=embeddings_file_path, \n",
    "                            embedding_name_list_file_path= \"data/embeddings/obj_files.json\",\n",
    "                            embedding_size=latent_size,\n",
    "                            image_count=image_count,\n",
    "                            sample_count=sample_count,\n",
    "                            dynamic=is_dynamic,\n",
    "                            transform=transforms.Compose([\n",
    "                                T.Resize(224,224, image_count),\n",
    "                                OmniObject.ToTensor(divide255=True, img_count=image_count)\n",
    "                                ])\n",
    "                            )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(latent_size, aggregation_simple=aggregation_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model.load_state_dict(torch.load('MAE_loss_checkpoints/model_epoch_290.pth', map_location=device))\n",
    "# optimizer.load_state_dict(torch.load('MAE_loss_checkpoints/optimizer_epoch_290.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "OUTPUT_PATH = \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch: 1/1000  Batch: 276/276  Epoch_Loss: 0.00138119  Batch_Loss:0.00139377\n",
      "[TRAIN]  Epoch: 1  Loss: 0.00138119\n",
      "[VAL]  Epoch: 1/1000  Batch: 28/28  Epoch_Loss: 0.00131337  Batch_Loss:0.00112267\n",
      "[VAL]  Epoch: 1  Loss: 0.00131337\n",
      "[TRAIN]  Epoch: 2/1000  Batch: 276/276  Epoch_Loss: 0.00131538  Batch_Loss:0.00102183\n",
      "[TRAIN]  Epoch: 2  Loss: 0.00131538\n",
      "[VAL]  Epoch: 2/1000  Batch: 28/28  Epoch_Loss: 0.00143432  Batch_Loss:0.00155397\n",
      "[VAL]  Epoch: 2  Loss: 0.00143432\n",
      "[TRAIN]  Epoch: 3/1000  Batch: 276/276  Epoch_Loss: 0.00133168  Batch_Loss:0.00127095\n",
      "[TRAIN]  Epoch: 3  Loss: 0.00133168\n",
      "[VAL]  Epoch: 3/1000  Batch: 28/28  Epoch_Loss: 0.00132889  Batch_Loss:0.00122962\n",
      "[VAL]  Epoch: 3  Loss: 0.00132889\n",
      "[TRAIN]  Epoch: 4/1000  Batch: 276/276  Epoch_Loss: 0.00127172  Batch_Loss:0.00112312\n",
      "[TRAIN]  Epoch: 4  Loss: 0.00127172\n",
      "[VAL]  Epoch: 4/1000  Batch: 28/28  Epoch_Loss: 0.00129975  Batch_Loss:0.00113544\n",
      "[VAL]  Epoch: 4  Loss: 0.00129975\n",
      "[TRAIN]  Epoch: 5/1000  Batch: 276/276  Epoch_Loss: 0.00127131  Batch_Loss:0.00135300\n",
      "[TRAIN]  Epoch: 5  Loss: 0.00127131\n",
      "[VAL]  Epoch: 5/1000  Batch: 28/28  Epoch_Loss: 0.00129422  Batch_Loss:0.00113932\n",
      "[VAL]  Epoch: 5  Loss: 0.00129422\n",
      "[TRAIN]  Epoch: 6/1000  Batch: 276/276  Epoch_Loss: 0.00123372  Batch_Loss:0.00090500\n",
      "[TRAIN]  Epoch: 6  Loss: 0.00123372\n",
      "[VAL]  Epoch: 6/1000  Batch: 28/28  Epoch_Loss: 0.00125110  Batch_Loss:0.00111957\n",
      "[VAL]  Epoch: 6  Loss: 0.00125110\n",
      "[TRAIN]  Epoch: 7/1000  Batch: 276/276  Epoch_Loss: 0.00122265  Batch_Loss:0.00138314\n",
      "[TRAIN]  Epoch: 7  Loss: 0.00122265\n",
      "[VAL]  Epoch: 7/1000  Batch: 28/28  Epoch_Loss: 0.00123769  Batch_Loss:0.00112396\n",
      "[VAL]  Epoch: 7  Loss: 0.00123769\n",
      "[TRAIN]  Epoch: 8/1000  Batch: 276/276  Epoch_Loss: 0.00122577  Batch_Loss:0.00129381\n",
      "[TRAIN]  Epoch: 8  Loss: 0.00122577\n",
      "[VAL]  Epoch: 8/1000  Batch: 28/28  Epoch_Loss: 0.00124086  Batch_Loss:0.00127001\n",
      "[VAL]  Epoch: 8  Loss: 0.00124086\n",
      "[TRAIN]  Epoch: 9/1000  Batch: 276/276  Epoch_Loss: 0.00120596  Batch_Loss:0.00095451\n",
      "[TRAIN]  Epoch: 9  Loss: 0.00120596\n",
      "[VAL]  Epoch: 9/1000  Batch: 28/28  Epoch_Loss: 0.00119885  Batch_Loss:0.00117604\n",
      "[VAL]  Epoch: 9  Loss: 0.00119885\n",
      "[TRAIN]  Epoch: 10/1000  Batch: 276/276  Epoch_Loss: 0.00122049  Batch_Loss:0.00120255\n",
      "[TRAIN]  Epoch: 10  Loss: 0.00122049\n",
      "[VAL]  Epoch: 10/1000  Batch: 28/28  Epoch_Loss: 0.00124225  Batch_Loss:0.00106363\n",
      "[VAL]  Epoch: 10  Loss: 0.00124225\n",
      "[TRAIN]  Epoch: 11/1000  Batch: 276/276  Epoch_Loss: 0.00120842  Batch_Loss:0.00112750\n",
      "[TRAIN]  Epoch: 11  Loss: 0.00120842\n",
      "[VAL]  Epoch: 11/1000  Batch: 28/28  Epoch_Loss: 0.00120815  Batch_Loss:0.00111180\n",
      "[VAL]  Epoch: 11  Loss: 0.00120815\n",
      "[TRAIN]  Epoch: 12/1000  Batch: 276/276  Epoch_Loss: 0.00118827  Batch_Loss:0.00061026\n",
      "[TRAIN]  Epoch: 12  Loss: 0.00118827\n",
      "[VAL]  Epoch: 12/1000  Batch: 28/28  Epoch_Loss: 0.00120672  Batch_Loss:0.00115797\n",
      "[VAL]  Epoch: 12  Loss: 0.00120672\n",
      "[TRAIN]  Epoch: 13/1000  Batch: 276/276  Epoch_Loss: 0.00119122  Batch_Loss:0.00145532\n",
      "[TRAIN]  Epoch: 13  Loss: 0.00119122\n",
      "[VAL]  Epoch: 13/1000  Batch: 28/28  Epoch_Loss: 0.00119581  Batch_Loss:0.00117380\n",
      "[VAL]  Epoch: 13  Loss: 0.00119581\n",
      "[TRAIN]  Epoch: 14/1000  Batch: 276/276  Epoch_Loss: 0.00118233  Batch_Loss:0.00094901\n",
      "[TRAIN]  Epoch: 14  Loss: 0.00118233\n",
      "[VAL]  Epoch: 14/1000  Batch: 28/28  Epoch_Loss: 0.00118282  Batch_Loss:0.00117302\n",
      "[VAL]  Epoch: 14  Loss: 0.00118282\n",
      "[TRAIN]  Epoch: 15/1000  Batch: 276/276  Epoch_Loss: 0.00117358  Batch_Loss:0.00097021\n",
      "[TRAIN]  Epoch: 15  Loss: 0.00117358\n",
      "[VAL]  Epoch: 15/1000  Batch: 28/28  Epoch_Loss: 0.00118391  Batch_Loss:0.00119492\n",
      "[VAL]  Epoch: 15  Loss: 0.00118391\n",
      "[TRAIN]  Epoch: 16/1000  Batch: 276/276  Epoch_Loss: 0.00116275  Batch_Loss:0.00109373\n",
      "[TRAIN]  Epoch: 16  Loss: 0.00116275\n",
      "[VAL]  Epoch: 16/1000  Batch: 28/28  Epoch_Loss: 0.00118792  Batch_Loss:0.00118246\n",
      "[VAL]  Epoch: 16  Loss: 0.00118792\n",
      "[TRAIN]  Epoch: 17/1000  Batch: 276/276  Epoch_Loss: 0.00117870  Batch_Loss:0.00101674\n",
      "[TRAIN]  Epoch: 17  Loss: 0.00117870\n",
      "[VAL]  Epoch: 17/1000  Batch: 28/28  Epoch_Loss: 0.00123458  Batch_Loss:0.00124790\n",
      "[VAL]  Epoch: 17  Loss: 0.00123458\n",
      "[TRAIN]  Epoch: 18/1000  Batch: 276/276  Epoch_Loss: 0.00117006  Batch_Loss:0.00120581\n",
      "[TRAIN]  Epoch: 18  Loss: 0.00117006\n",
      "[VAL]  Epoch: 18/1000  Batch: 28/28  Epoch_Loss: 0.00119820  Batch_Loss:0.00120038\n",
      "[VAL]  Epoch: 18  Loss: 0.00119820\n",
      "[TRAIN]  Epoch: 19/1000  Batch: 276/276  Epoch_Loss: 0.00116492  Batch_Loss:0.00113413\n",
      "[TRAIN]  Epoch: 19  Loss: 0.00116492\n",
      "[VAL]  Epoch: 19/1000  Batch: 28/28  Epoch_Loss: 0.00121719  Batch_Loss:0.00125228\n",
      "[VAL]  Epoch: 19  Loss: 0.00121719\n",
      "[TRAIN]  Epoch: 20/1000  Batch: 276/276  Epoch_Loss: 0.00116065  Batch_Loss:0.00095767\n",
      "[TRAIN]  Epoch: 20  Loss: 0.00116065\n",
      "[VAL]  Epoch: 20/1000  Batch: 28/28  Epoch_Loss: 0.00118928  Batch_Loss:0.00106735\n",
      "[VAL]  Epoch: 20  Loss: 0.00118928\n",
      "[TRAIN]  Epoch: 21/1000  Batch: 276/276  Epoch_Loss: 0.00115833  Batch_Loss:0.00122883\n",
      "[TRAIN]  Epoch: 21  Loss: 0.00115833\n",
      "[VAL]  Epoch: 21/1000  Batch: 28/28  Epoch_Loss: 0.00138293  Batch_Loss:0.00154840\n",
      "[VAL]  Epoch: 21  Loss: 0.00138293\n",
      "[TRAIN]  Epoch: 22/1000  Batch: 276/276  Epoch_Loss: 0.00117075  Batch_Loss:0.00141787\n",
      "[TRAIN]  Epoch: 22  Loss: 0.00117075\n",
      "[VAL]  Epoch: 22/1000  Batch: 28/28  Epoch_Loss: 0.00116066  Batch_Loss:0.00124223\n",
      "[VAL]  Epoch: 22  Loss: 0.00116066\n",
      "[TRAIN]  Epoch: 23/1000  Batch: 276/276  Epoch_Loss: 0.00114739  Batch_Loss:0.00105062\n",
      "[TRAIN]  Epoch: 23  Loss: 0.00114739\n",
      "[VAL]  Epoch: 23/1000  Batch: 28/28  Epoch_Loss: 0.00114680  Batch_Loss:0.00118394\n",
      "[VAL]  Epoch: 23  Loss: 0.00114680\n",
      "[TRAIN]  Epoch: 24/1000  Batch: 276/276  Epoch_Loss: 0.00114431  Batch_Loss:0.00139301\n",
      "[TRAIN]  Epoch: 24  Loss: 0.00114431\n",
      "[VAL]  Epoch: 24/1000  Batch: 28/28  Epoch_Loss: 0.00115185  Batch_Loss:0.00115998\n",
      "[VAL]  Epoch: 24  Loss: 0.00115185\n",
      "[TRAIN]  Epoch: 25/1000  Batch: 276/276  Epoch_Loss: 0.00113855  Batch_Loss:0.00121019\n",
      "[TRAIN]  Epoch: 25  Loss: 0.00113855\n",
      "[VAL]  Epoch: 25/1000  Batch: 28/28  Epoch_Loss: 0.00112892  Batch_Loss:0.00100917\n",
      "[VAL]  Epoch: 25  Loss: 0.00112892\n",
      "[TRAIN]  Epoch: 26/1000  Batch: 276/276  Epoch_Loss: 0.00114534  Batch_Loss:0.00098230\n",
      "[TRAIN]  Epoch: 26  Loss: 0.00114534\n",
      "[VAL]  Epoch: 26/1000  Batch: 28/28  Epoch_Loss: 0.00115284  Batch_Loss:0.00116809\n",
      "[VAL]  Epoch: 26  Loss: 0.00115284\n",
      "[TRAIN]  Epoch: 27/1000  Batch: 276/276  Epoch_Loss: 0.00115003  Batch_Loss:0.00125166\n",
      "[TRAIN]  Epoch: 27  Loss: 0.00115003\n",
      "[VAL]  Epoch: 27/1000  Batch: 28/28  Epoch_Loss: 0.00115832  Batch_Loss:0.00105283\n",
      "[VAL]  Epoch: 27  Loss: 0.00115832\n",
      "[TRAIN]  Epoch: 28/1000  Batch: 276/276  Epoch_Loss: 0.00114591  Batch_Loss:0.00118039\n",
      "[TRAIN]  Epoch: 28  Loss: 0.00114591\n",
      "[VAL]  Epoch: 28/1000  Batch: 28/28  Epoch_Loss: 0.00111724  Batch_Loss:0.00119556\n",
      "[VAL]  Epoch: 28  Loss: 0.00111724\n",
      "[TRAIN]  Epoch: 29/1000  Batch: 276/276  Epoch_Loss: 0.00116035  Batch_Loss:0.00109424\n",
      "[TRAIN]  Epoch: 29  Loss: 0.00116035\n",
      "[VAL]  Epoch: 29/1000  Batch: 28/28  Epoch_Loss: 0.00112311  Batch_Loss:0.00097745\n",
      "[VAL]  Epoch: 29  Loss: 0.00112311\n",
      "[TRAIN]  Epoch: 30/1000  Batch: 276/276  Epoch_Loss: 0.00112582  Batch_Loss:0.00082300\n",
      "[TRAIN]  Epoch: 30  Loss: 0.00112582\n",
      "[VAL]  Epoch: 30/1000  Batch: 28/28  Epoch_Loss: 0.00113417  Batch_Loss:0.00099314\n",
      "[VAL]  Epoch: 30  Loss: 0.00113417\n",
      "[TRAIN]  Epoch: 31/1000  Batch: 276/276  Epoch_Loss: 0.00112359  Batch_Loss:0.00084823\n",
      "[TRAIN]  Epoch: 31  Loss: 0.00112359\n",
      "[VAL]  Epoch: 31/1000  Batch: 28/28  Epoch_Loss: 0.00109648  Batch_Loss:0.00097119\n",
      "[VAL]  Epoch: 31  Loss: 0.00109648\n",
      "[TRAIN]  Epoch: 32/1000  Batch: 276/276  Epoch_Loss: 0.00110402  Batch_Loss:0.00079290\n",
      "[TRAIN]  Epoch: 32  Loss: 0.00110402\n",
      "[VAL]  Epoch: 32/1000  Batch: 28/28  Epoch_Loss: 0.00109198  Batch_Loss:0.00099158\n",
      "[VAL]  Epoch: 32  Loss: 0.00109198\n",
      "[TRAIN]  Epoch: 33/1000  Batch: 276/276  Epoch_Loss: 0.00110682  Batch_Loss:0.00127241\n",
      "[TRAIN]  Epoch: 33  Loss: 0.00110682\n",
      "[VAL]  Epoch: 33/1000  Batch: 28/28  Epoch_Loss: 0.00109356  Batch_Loss:0.00099842\n",
      "[VAL]  Epoch: 33  Loss: 0.00109356\n",
      "[TRAIN]  Epoch: 34/1000  Batch: 276/276  Epoch_Loss: 0.00111903  Batch_Loss:0.00100583\n",
      "[TRAIN]  Epoch: 34  Loss: 0.00111903\n",
      "[VAL]  Epoch: 34/1000  Batch: 28/28  Epoch_Loss: 0.00111701  Batch_Loss:0.00095029\n",
      "[VAL]  Epoch: 34  Loss: 0.00111701\n",
      "[TRAIN]  Epoch: 35/1000  Batch: 276/276  Epoch_Loss: 0.00111722  Batch_Loss:0.00136821\n",
      "[TRAIN]  Epoch: 35  Loss: 0.00111722\n",
      "[VAL]  Epoch: 35/1000  Batch: 28/28  Epoch_Loss: 0.00113590  Batch_Loss:0.00123809\n",
      "[VAL]  Epoch: 35  Loss: 0.00113590\n",
      "[TRAIN]  Epoch: 36/1000  Batch: 276/276  Epoch_Loss: 0.00110678  Batch_Loss:0.00160154\n",
      "[TRAIN]  Epoch: 36  Loss: 0.00110678\n",
      "[VAL]  Epoch: 36/1000  Batch: 28/28  Epoch_Loss: 0.00108660  Batch_Loss:0.00104908\n",
      "[VAL]  Epoch: 36  Loss: 0.00108660\n",
      "[TRAIN]  Epoch: 37/1000  Batch: 276/276  Epoch_Loss: 0.00109994  Batch_Loss:0.00073195\n",
      "[TRAIN]  Epoch: 37  Loss: 0.00109994\n",
      "[VAL]  Epoch: 37/1000  Batch: 28/28  Epoch_Loss: 0.00111796  Batch_Loss:0.00130415\n",
      "[VAL]  Epoch: 37  Loss: 0.00111796\n",
      "[TRAIN]  Epoch: 38/1000  Batch: 276/276  Epoch_Loss: 0.00108998  Batch_Loss:0.00102042\n",
      "[TRAIN]  Epoch: 38  Loss: 0.00108998\n",
      "[VAL]  Epoch: 38/1000  Batch: 28/28  Epoch_Loss: 0.00107617  Batch_Loss:0.00095567\n",
      "[VAL]  Epoch: 38  Loss: 0.00107617\n",
      "[TRAIN]  Epoch: 39/1000  Batch: 276/276  Epoch_Loss: 0.00108710  Batch_Loss:0.00101459\n",
      "[TRAIN]  Epoch: 39  Loss: 0.00108710\n",
      "[VAL]  Epoch: 39/1000  Batch: 28/28  Epoch_Loss: 0.00107304  Batch_Loss:0.00095213\n",
      "[VAL]  Epoch: 39  Loss: 0.00107304\n",
      "[TRAIN]  Epoch: 40/1000  Batch: 276/276  Epoch_Loss: 0.00115474  Batch_Loss:0.00132207\n",
      "[TRAIN]  Epoch: 40  Loss: 0.00115474\n",
      "[VAL]  Epoch: 40/1000  Batch: 28/28  Epoch_Loss: 0.00111086  Batch_Loss:0.00111206\n",
      "[VAL]  Epoch: 40  Loss: 0.00111086\n",
      "[TRAIN]  Epoch: 41/1000  Batch: 276/276  Epoch_Loss: 0.00110892  Batch_Loss:0.00155324\n",
      "[TRAIN]  Epoch: 41  Loss: 0.00110892\n",
      "[VAL]  Epoch: 41/1000  Batch: 28/28  Epoch_Loss: 0.00111921  Batch_Loss:0.00114944\n",
      "[VAL]  Epoch: 41  Loss: 0.00111921\n",
      "[TRAIN]  Epoch: 42/1000  Batch: 276/276  Epoch_Loss: 0.00110698  Batch_Loss:0.00084936\n",
      "[TRAIN]  Epoch: 42  Loss: 0.00110698\n",
      "[VAL]  Epoch: 42/1000  Batch: 28/28  Epoch_Loss: 0.00110972  Batch_Loss:0.00095532\n",
      "[VAL]  Epoch: 42  Loss: 0.00110972\n",
      "[TRAIN]  Epoch: 43/1000  Batch: 276/276  Epoch_Loss: 0.00109221  Batch_Loss:0.00108968\n",
      "[TRAIN]  Epoch: 43  Loss: 0.00109221\n",
      "[VAL]  Epoch: 43/1000  Batch: 28/28  Epoch_Loss: 0.00110855  Batch_Loss:0.00099919\n",
      "[VAL]  Epoch: 43  Loss: 0.00110855\n",
      "[TRAIN]  Epoch: 44/1000  Batch: 276/276  Epoch_Loss: 0.00108375  Batch_Loss:0.00128024\n",
      "[TRAIN]  Epoch: 44  Loss: 0.00108375\n",
      "[VAL]  Epoch: 44/1000  Batch: 28/28  Epoch_Loss: 0.00108794  Batch_Loss:0.00121081\n",
      "[VAL]  Epoch: 44  Loss: 0.00108794\n",
      "[TRAIN]  Epoch: 45/1000  Batch: 276/276  Epoch_Loss: 0.00108482  Batch_Loss:0.00070463\n",
      "[TRAIN]  Epoch: 45  Loss: 0.00108482\n",
      "[VAL]  Epoch: 45/1000  Batch: 28/28  Epoch_Loss: 0.00109356  Batch_Loss:0.00095376\n",
      "[VAL]  Epoch: 45  Loss: 0.00109356\n",
      "[TRAIN]  Epoch: 46/1000  Batch: 276/276  Epoch_Loss: 0.00107783  Batch_Loss:0.00090067\n",
      "[TRAIN]  Epoch: 46  Loss: 0.00107783\n",
      "[VAL]  Epoch: 46/1000  Batch: 28/28  Epoch_Loss: 0.00104938  Batch_Loss:0.00104099\n",
      "[VAL]  Epoch: 46  Loss: 0.00104938\n",
      "[TRAIN]  Epoch: 47/1000  Batch: 276/276  Epoch_Loss: 0.00107346  Batch_Loss:0.00138881\n",
      "[TRAIN]  Epoch: 47  Loss: 0.00107346\n",
      "[VAL]  Epoch: 47/1000  Batch: 28/28  Epoch_Loss: 0.00107455  Batch_Loss:0.00114988\n",
      "[VAL]  Epoch: 47  Loss: 0.00107455\n",
      "[TRAIN]  Epoch: 48/1000  Batch: 276/276  Epoch_Loss: 0.00106712  Batch_Loss:0.00128394\n",
      "[TRAIN]  Epoch: 48  Loss: 0.00106712\n",
      "[VAL]  Epoch: 48/1000  Batch: 28/28  Epoch_Loss: 0.00109572  Batch_Loss:0.00100571\n",
      "[VAL]  Epoch: 48  Loss: 0.00109572\n",
      "[TRAIN]  Epoch: 49/1000  Batch: 276/276  Epoch_Loss: 0.00105729  Batch_Loss:0.00051695\n",
      "[TRAIN]  Epoch: 49  Loss: 0.00105729\n",
      "[VAL]  Epoch: 49/1000  Batch: 28/28  Epoch_Loss: 0.00110640  Batch_Loss:0.00099488\n",
      "[VAL]  Epoch: 49  Loss: 0.00110640\n",
      "[TRAIN]  Epoch: 50/1000  Batch: 276/276  Epoch_Loss: 0.00106989  Batch_Loss:0.00120422\n",
      "[TRAIN]  Epoch: 50  Loss: 0.00106989\n",
      "[VAL]  Epoch: 50/1000  Batch: 28/28  Epoch_Loss: 0.00107881  Batch_Loss:0.00110586\n",
      "[VAL]  Epoch: 50  Loss: 0.00107881\n",
      "[TRAIN]  Epoch: 51/1000  Batch: 276/276  Epoch_Loss: 0.00106210  Batch_Loss:0.00070775\n",
      "[TRAIN]  Epoch: 51  Loss: 0.00106210\n",
      "[VAL]  Epoch: 51/1000  Batch: 28/28  Epoch_Loss: 0.00106733  Batch_Loss:0.00095443\n",
      "[VAL]  Epoch: 51  Loss: 0.00106733\n",
      "[TRAIN]  Epoch: 52/1000  Batch: 276/276  Epoch_Loss: 0.00105885  Batch_Loss:0.00093046\n",
      "[TRAIN]  Epoch: 52  Loss: 0.00105885\n",
      "[VAL]  Epoch: 52/1000  Batch: 28/28  Epoch_Loss: 0.00107321  Batch_Loss:0.00119314\n",
      "[VAL]  Epoch: 52  Loss: 0.00107321\n",
      "[TRAIN]  Epoch: 53/1000  Batch: 276/276  Epoch_Loss: 0.00106836  Batch_Loss:0.00145746\n",
      "[TRAIN]  Epoch: 53  Loss: 0.00106836\n",
      "[VAL]  Epoch: 53/1000  Batch: 28/28  Epoch_Loss: 0.00107591  Batch_Loss:0.00101702\n",
      "[VAL]  Epoch: 53  Loss: 0.00107591\n",
      "[TRAIN]  Epoch: 54/1000  Batch: 276/276  Epoch_Loss: 0.00106257  Batch_Loss:0.00153918\n",
      "[TRAIN]  Epoch: 54  Loss: 0.00106257\n",
      "[VAL]  Epoch: 54/1000  Batch: 28/28  Epoch_Loss: 0.00109179  Batch_Loss:0.00103152\n",
      "[VAL]  Epoch: 54  Loss: 0.00109179\n",
      "[TRAIN]  Epoch: 55/1000  Batch: 276/276  Epoch_Loss: 0.00104895  Batch_Loss:0.00094304\n",
      "[TRAIN]  Epoch: 55  Loss: 0.00104895\n",
      "[VAL]  Epoch: 55/1000  Batch: 28/28  Epoch_Loss: 0.00106049  Batch_Loss:0.00091543\n",
      "[VAL]  Epoch: 55  Loss: 0.00106049\n",
      "[TRAIN]  Epoch: 56/1000  Batch: 276/276  Epoch_Loss: 0.00106135  Batch_Loss:0.00080790\n",
      "[TRAIN]  Epoch: 56  Loss: 0.00106135\n",
      "[VAL]  Epoch: 56/1000  Batch: 28/28  Epoch_Loss: 0.00107807  Batch_Loss:0.00111730\n",
      "[VAL]  Epoch: 56  Loss: 0.00107807\n",
      "[TRAIN]  Epoch: 57/1000  Batch: 276/276  Epoch_Loss: 0.00105484  Batch_Loss:0.00079712\n",
      "[TRAIN]  Epoch: 57  Loss: 0.00105484\n",
      "[VAL]  Epoch: 57/1000  Batch: 28/28  Epoch_Loss: 0.00108499  Batch_Loss:0.00103906\n",
      "[VAL]  Epoch: 57  Loss: 0.00108499\n",
      "[TRAIN]  Epoch: 58/1000  Batch: 276/276  Epoch_Loss: 0.00106199  Batch_Loss:0.00065424\n",
      "[TRAIN]  Epoch: 58  Loss: 0.00106199\n",
      "[VAL]  Epoch: 58/1000  Batch: 28/28  Epoch_Loss: 0.00108180  Batch_Loss:0.00092612\n",
      "[VAL]  Epoch: 58  Loss: 0.00108180\n",
      "[TRAIN]  Epoch: 59/1000  Batch: 276/276  Epoch_Loss: 0.00106174  Batch_Loss:0.00098686\n",
      "[TRAIN]  Epoch: 59  Loss: 0.00106174\n",
      "[VAL]  Epoch: 59/1000  Batch: 28/28  Epoch_Loss: 0.00109147  Batch_Loss:0.00092368\n",
      "[VAL]  Epoch: 59  Loss: 0.00109147\n",
      "[TRAIN]  Epoch: 60/1000  Batch: 276/276  Epoch_Loss: 0.00105374  Batch_Loss:0.00070472\n",
      "[TRAIN]  Epoch: 60  Loss: 0.00105374\n",
      "[VAL]  Epoch: 60/1000  Batch: 28/28  Epoch_Loss: 0.00105179  Batch_Loss:0.00089911\n",
      "[VAL]  Epoch: 60  Loss: 0.00105179\n",
      "[TRAIN]  Epoch: 61/1000  Batch: 276/276  Epoch_Loss: 0.00105757  Batch_Loss:0.00130761\n",
      "[TRAIN]  Epoch: 61  Loss: 0.00105757\n",
      "[VAL]  Epoch: 61/1000  Batch: 28/28  Epoch_Loss: 0.00108777  Batch_Loss:0.00096387\n",
      "[VAL]  Epoch: 61  Loss: 0.00108777\n",
      "[TRAIN]  Epoch: 62/1000  Batch: 276/276  Epoch_Loss: 0.00104081  Batch_Loss:0.00094030\n",
      "[TRAIN]  Epoch: 62  Loss: 0.00104081\n",
      "[VAL]  Epoch: 62/1000  Batch: 28/28  Epoch_Loss: 0.00105383  Batch_Loss:0.00103582\n",
      "[VAL]  Epoch: 62  Loss: 0.00105383\n",
      "[TRAIN]  Epoch: 63/1000  Batch: 276/276  Epoch_Loss: 0.00104123  Batch_Loss:0.00158861\n",
      "[TRAIN]  Epoch: 63  Loss: 0.00104123\n",
      "[VAL]  Epoch: 63/1000  Batch: 28/28  Epoch_Loss: 0.00104073  Batch_Loss:0.00100700\n",
      "[VAL]  Epoch: 63  Loss: 0.00104073\n",
      "[TRAIN]  Epoch: 64/1000  Batch: 276/276  Epoch_Loss: 0.00104953  Batch_Loss:0.00149625\n",
      "[TRAIN]  Epoch: 64  Loss: 0.00104953\n",
      "[VAL]  Epoch: 64/1000  Batch: 28/28  Epoch_Loss: 0.00105999  Batch_Loss:0.00094717\n",
      "[VAL]  Epoch: 64  Loss: 0.00105999\n",
      "[TRAIN]  Epoch: 65/1000  Batch: 276/276  Epoch_Loss: 0.00105179  Batch_Loss:0.00068621\n",
      "[TRAIN]  Epoch: 65  Loss: 0.00105179\n",
      "[VAL]  Epoch: 65/1000  Batch: 28/28  Epoch_Loss: 0.00104544  Batch_Loss:0.00093475\n",
      "[VAL]  Epoch: 65  Loss: 0.00104544\n",
      "[TRAIN]  Epoch: 66/1000  Batch: 276/276  Epoch_Loss: 0.00104937  Batch_Loss:0.00090810\n",
      "[TRAIN]  Epoch: 66  Loss: 0.00104937\n",
      "[VAL]  Epoch: 66/1000  Batch: 28/28  Epoch_Loss: 0.00107569  Batch_Loss:0.00102570\n",
      "[VAL]  Epoch: 66  Loss: 0.00107569\n",
      "[TRAIN]  Epoch: 67/1000  Batch: 276/276  Epoch_Loss: 0.00220510  Batch_Loss:0.00170739\n",
      "[TRAIN]  Epoch: 67  Loss: 0.00220510\n",
      "[VAL]  Epoch: 67/1000  Batch: 28/28  Epoch_Loss: 0.00142779  Batch_Loss:0.00135122\n",
      "[VAL]  Epoch: 67  Loss: 0.00142779\n",
      "[TRAIN]  Epoch: 68/1000  Batch: 276/276  Epoch_Loss: 0.00139179  Batch_Loss:0.00151638\n",
      "[TRAIN]  Epoch: 68  Loss: 0.00139179\n",
      "[VAL]  Epoch: 68/1000  Batch: 28/28  Epoch_Loss: 0.00140597  Batch_Loss:0.00131621\n",
      "[VAL]  Epoch: 68  Loss: 0.00140597\n",
      "[TRAIN]  Epoch: 69/1000  Batch: 276/276  Epoch_Loss: 0.00137790  Batch_Loss:0.00168316\n",
      "[TRAIN]  Epoch: 69  Loss: 0.00137790\n",
      "[VAL]  Epoch: 69/1000  Batch: 28/28  Epoch_Loss: 0.00140258  Batch_Loss:0.00130775\n",
      "[VAL]  Epoch: 69  Loss: 0.00140258\n",
      "[TRAIN]  Epoch: 70/1000  Batch: 276/276  Epoch_Loss: 0.00137758  Batch_Loss:0.00147679\n",
      "[TRAIN]  Epoch: 70  Loss: 0.00137758\n",
      "[VAL]  Epoch: 70/1000  Batch: 28/28  Epoch_Loss: 0.00140149  Batch_Loss:0.00134734\n",
      "[VAL]  Epoch: 70  Loss: 0.00140149\n",
      "[TRAIN]  Epoch: 71/1000  Batch: 276/276  Epoch_Loss: 0.00137772  Batch_Loss:0.00138244\n",
      "[TRAIN]  Epoch: 71  Loss: 0.00137772\n",
      "[VAL]  Epoch: 71/1000  Batch: 28/28  Epoch_Loss: 0.00140119  Batch_Loss:0.00132887\n",
      "[VAL]  Epoch: 71  Loss: 0.00140119\n",
      "[TRAIN]  Epoch: 72/1000  Batch: 276/276  Epoch_Loss: 0.00137732  Batch_Loss:0.00174470\n",
      "[TRAIN]  Epoch: 72  Loss: 0.00137732\n",
      "[VAL]  Epoch: 72/1000  Batch: 28/28  Epoch_Loss: 0.00140705  Batch_Loss:0.00128819\n",
      "[VAL]  Epoch: 72  Loss: 0.00140705\n",
      "[TRAIN]  Epoch: 73/1000  Batch: 276/276  Epoch_Loss: 0.00137756  Batch_Loss:0.00128857\n",
      "[TRAIN]  Epoch: 73  Loss: 0.00137756\n",
      "[VAL]  Epoch: 73/1000  Batch: 28/28  Epoch_Loss: 0.00140671  Batch_Loss:0.00136575\n",
      "[VAL]  Epoch: 73  Loss: 0.00140671\n",
      "[TRAIN]  Epoch: 74/1000  Batch: 276/276  Epoch_Loss: 0.00137790  Batch_Loss:0.00123809\n",
      "[TRAIN]  Epoch: 74  Loss: 0.00137790\n",
      "[VAL]  Epoch: 74/1000  Batch: 28/28  Epoch_Loss: 0.00140237  Batch_Loss:0.00136530\n",
      "[VAL]  Epoch: 74  Loss: 0.00140237\n",
      "[TRAIN]  Epoch: 75/1000  Batch: 276/276  Epoch_Loss: 0.00137606  Batch_Loss:0.00160698\n",
      "[TRAIN]  Epoch: 75  Loss: 0.00137606\n",
      "[VAL]  Epoch: 75/1000  Batch: 28/28  Epoch_Loss: 0.00140300  Batch_Loss:0.00131333\n",
      "[VAL]  Epoch: 75  Loss: 0.00140300\n",
      "[TRAIN]  Epoch: 76/1000  Batch: 276/276  Epoch_Loss: 0.00137744  Batch_Loss:0.00161377\n",
      "[TRAIN]  Epoch: 76  Loss: 0.00137744\n",
      "[VAL]  Epoch: 76/1000  Batch: 28/28  Epoch_Loss: 0.00140223  Batch_Loss:0.00129674\n",
      "[VAL]  Epoch: 76  Loss: 0.00140223\n",
      "[TRAIN]  Epoch: 77/1000  Batch: 276/276  Epoch_Loss: 0.00137699  Batch_Loss:0.00089779\n",
      "[TRAIN]  Epoch: 77  Loss: 0.00137699\n",
      "[VAL]  Epoch: 77/1000  Batch: 28/28  Epoch_Loss: 0.00140207  Batch_Loss:0.00134813\n",
      "[VAL]  Epoch: 77  Loss: 0.00140207\n",
      "[TRAIN]  Epoch: 78/1000  Batch: 276/276  Epoch_Loss: 0.00137778  Batch_Loss:0.00134078\n",
      "[TRAIN]  Epoch: 78  Loss: 0.00137778\n",
      "[VAL]  Epoch: 78/1000  Batch: 28/28  Epoch_Loss: 0.00140572  Batch_Loss:0.00132927\n",
      "[VAL]  Epoch: 78  Loss: 0.00140572\n",
      "[TRAIN]  Epoch: 79/1000  Batch: 276/276  Epoch_Loss: 0.00137716  Batch_Loss:0.00137284\n",
      "[TRAIN]  Epoch: 79  Loss: 0.00137716\n",
      "[VAL]  Epoch: 79/1000  Batch: 28/28  Epoch_Loss: 0.00140155  Batch_Loss:0.00134823\n",
      "[VAL]  Epoch: 79  Loss: 0.00140155\n",
      "[TRAIN]  Epoch: 80/1000  Batch: 276/276  Epoch_Loss: 0.00137799  Batch_Loss:0.00132039\n",
      "[TRAIN]  Epoch: 80  Loss: 0.00137799\n",
      "[VAL]  Epoch: 80/1000  Batch: 28/28  Epoch_Loss: 0.00140291  Batch_Loss:0.00133747\n",
      "[VAL]  Epoch: 80  Loss: 0.00140291\n",
      "[TRAIN]  Epoch: 81/1000  Batch: 276/276  Epoch_Loss: 0.00137747  Batch_Loss:0.00137112\n",
      "[TRAIN]  Epoch: 81  Loss: 0.00137747\n",
      "[VAL]  Epoch: 81/1000  Batch: 28/28  Epoch_Loss: 0.00140879  Batch_Loss:0.00132530\n",
      "[VAL]  Epoch: 81  Loss: 0.00140879\n",
      "[TRAIN]  Epoch: 82/1000  Batch: 276/276  Epoch_Loss: 0.00138813  Batch_Loss:0.00128522\n",
      "[TRAIN]  Epoch: 82  Loss: 0.00138813\n",
      "[VAL]  Epoch: 82/1000  Batch: 28/28  Epoch_Loss: 0.00140169  Batch_Loss:0.00135851\n",
      "[VAL]  Epoch: 82  Loss: 0.00140169\n",
      "[TRAIN]  Epoch: 83/1000  Batch: 276/276  Epoch_Loss: 0.00137791  Batch_Loss:0.00133225\n",
      "[TRAIN]  Epoch: 83  Loss: 0.00137791\n",
      "[VAL]  Epoch: 83/1000  Batch: 28/28  Epoch_Loss: 0.00140479  Batch_Loss:0.00135516\n",
      "[VAL]  Epoch: 83  Loss: 0.00140479\n",
      "[TRAIN]  Epoch: 84/1000  Batch: 276/276  Epoch_Loss: 0.00137782  Batch_Loss:0.00107078\n",
      "[TRAIN]  Epoch: 84  Loss: 0.00137782\n",
      "[VAL]  Epoch: 84/1000  Batch: 28/28  Epoch_Loss: 0.00140506  Batch_Loss:0.00129289\n",
      "[VAL]  Epoch: 84  Loss: 0.00140506\n",
      "[TRAIN]  Epoch: 85/1000  Batch: 276/276  Epoch_Loss: 0.00137751  Batch_Loss:0.00158560\n",
      "[TRAIN]  Epoch: 85  Loss: 0.00137751\n",
      "[VAL]  Epoch: 85/1000  Batch: 14/28  Epoch_Loss: 0.00144351  Batch_Loss:0.00148477"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTRAIN\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[39m=\u001b[39m model(img_models) \u001b[39m# (N*T, 512)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m#DebugImages(i, img_models)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(outputs, (N,T,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m# (N, T, 512)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\TUM\\ML43D_Project\\3d-lmnet-pytorch\\Project\\model.py:46\u001b[0m, in \u001b[0;36mEfficientNetModified.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 46\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 333\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m    335\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    336\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_res_connect:\n\u001b[0;32m    166\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phases = {\"TRAIN\":train_loader, \"VAL\":val_loader}\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS):\n",
    "\n",
    "    for phase, data_loader in phases.items():\n",
    "        model.train() if phase == \"TRAIN\" else model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        img_model_counter = 0\n",
    "\n",
    "        for i, batch in enumerate(data_loader, start=1): \n",
    "            batch[\"imgs\"] = batch[\"imgs\"].to(device)\n",
    "            batch[\"embeddings\"] = batch[\"embeddings\"].to(device)\n",
    "\n",
    "            img_models, embeddings = batch[\"imgs\"], batch[\"embeddings\"]\n",
    "            img_model_counter += img_models.shape[0]\n",
    "\n",
    "            N, T = img_models.shape[0], img_models.shape[1]\n",
    "            img_models = torch.flatten(img_models, start_dim=0, end_dim=1)\n",
    "\n",
    "            img_models = img_models.to(device)\n",
    "            embeddings = embeddings.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"TRAIN\"):\n",
    "                if phase == \"TRAIN\":\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(img_models) # (N*T, 512)\n",
    "                #DebugImages(i, img_models)\n",
    "                outputs = torch.reshape(outputs, (N,T,-1)) # (N, T, 512)\n",
    "                if aggregation_simple:\n",
    "                    pred_embeds = torch.mean(outputs, axis=1) \n",
    "                else:\n",
    "                    pred_embeds = model.aggregate(outputs)\n",
    "                \n",
    "                loss = criterion(pred_embeds, embeddings)\n",
    "                if phase == \"TRAIN\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * N\n",
    "            \n",
    "            print(f\"\\r[{phase}]  Epoch: {epoch}/{NUM_EPOCHS}  Batch: {i}/{len(data_loader)}  Epoch_Loss: {running_loss/img_model_counter:.8f}  Batch_Loss:{loss.item():.8f}\", end=\"\")\n",
    "            \n",
    "        print(f\"\\n[{phase}]  Epoch: {epoch}  Loss: {running_loss/img_model_counter:.8f}\")\n",
    "        \n",
    "        if phase == \"TRAIN\" : \n",
    "            Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), Path(OUTPUT_PATH, f\"model_epoch_{epoch}.pth\"))\n",
    "            torch.save(optimizer.state_dict(), Path(OUTPUT_PATH, f\"optimizer_epoch_{epoch}.pth\"))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
